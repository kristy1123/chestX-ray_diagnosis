import os
import multiprocessing

# [ì¤‘ìš”] CPU ì‚¬ìš©ë¥  100%ë¥¼ ìœ„í•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (PyTorch ì„í¬íŠ¸ ì „ì— ì„¤ì • ê¶Œì¥)
# WSLì—ì„œ ë¬¼ë¦¬ ì½”ì–´ë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ë„ë¡ ê°•ì œí•©ë‹ˆë‹¤.
num_cores = multiprocessing.cpu_count()
os.environ["OMP_NUM_THREADS"] = str(num_cores)
os.environ["MKL_NUM_THREADS"] = str(num_cores)
os.environ["TORCH_NUM_THREADS"] = str(num_cores)

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
import pandas as pd
import numpy as np
import cv2
from PIL import Image
from sklearn.metrics import roc_auc_score
from sklearn.utils import shuffle
from tqdm import tqdm
from sklearn.model_selection import train_test_split

# ì‹œê°í™” ê´€ë ¨
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import metrics  # [ìˆ˜ì •] ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•´ ëª¨ë“ˆ ì „ì²´ ì„í¬íŠ¸



# PyTorch ë‚´ë¶€ ìŠ¤ë ˆë“œ ì„¤ì •
torch.set_num_threads(num_cores)


# =========================================================
# [1] ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„í•  í•¨ìˆ˜ (ë¡œì§ ìˆ˜ì •ë¨)
# =========================================================
def prepare_datasets(csv_path, target_classes):
    print(f"ğŸ“‚ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘: {csv_path}")
    df = pd.read_csv(csv_path)
    
    # 1. ì „ì²´ ë°ì´í„°ì—ì„œ íƒ€ê²Ÿ ì§ˆë³‘ê³¼ ê´€ë ¨ëœ ë°ì´í„°ë§Œ ì¼ë‹¨ ì¶”ë¦½ë‹ˆë‹¤.
    # (No Finding í¬í•¨, íƒ€ê²Ÿ ì§ˆë³‘ ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ëœ ëª¨ë“  í–‰)
    condition = df['Finding Labels'].str.contains(target_classes[0])
    for label in target_classes[1:]:
        condition |= df['Finding Labels'].str.contains(label)
    
    filtered_df = df[condition].copy()
    
    # 2. [í•µì‹¬ ë³€ê²½] "ì´ë¯¸ì§€ ID" ê¸°ì¤€ìœ¼ë¡œ Train/Testë¥¼ ë¨¼ì € ë‚˜ëˆ•ë‹ˆë‹¤ (8:2).
    # ì´ë ‡ê²Œ í•´ì•¼ í¬ê·€ ì§ˆí™˜ë„ ë¹„ìœ¨ëŒ€ë¡œ í…ŒìŠ¤íŠ¸ì…‹ì— ë“¤ì–´ê°‘ë‹ˆë‹¤.
    all_indices = filtered_df['Image Index'].unique()
    train_ids, test_ids = train_test_split(all_indices, test_size=0.2, random_state=42)
    
    # ID ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°í”„ë ˆì„ ë¶„ë¦¬
    raw_train_df = filtered_df[filtered_df['Image Index'].isin(train_ids)]
    test_df = filtered_df[filtered_df['Image Index'].isin(test_ids)].reset_index(drop=True)
    
    print(f"ğŸ“Œ ì „ì²´ ë°ì´í„° ë¶„í•  ì™„ë£Œ: Train í›„ë³´ {len(raw_train_df)}ì¥ / Test í™•ì • {len(test_df)}ì¥")

    # ---------------------------------------------------------
    # 3. Train ë°ì´í„°ì…‹ ë‚´ë¶€ ë°¸ëŸ°ì‹± (2:1:1:1) ìˆ˜í–‰
    # ---------------------------------------------------------
    dfs = {}
    for label in target_classes:
        dfs[label] = raw_train_df[raw_train_df['Finding Labels'].str.contains(label)]
    
    # No Findingì„ ì œì™¸í•œ ì§ˆë³‘ë“¤ì˜ ë°ì´í„° ìˆ˜ í™•ì¸
    disease_counts = [len(dfs[c]) for c in target_classes if c != 'No Finding']
    
    if not disease_counts:
        raise ValueError("í•™ìŠµ ë°ì´í„°ì…‹ í›„ë³´êµ°ì— ì§ˆë³‘ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
    # ê°€ì¥ ì ì€ ì§ˆë³‘ ë°ì´í„° ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë§ì¶¤
    min_count = min(disease_counts)
    
    # ë°ì´í„°ê°€ ë„ˆë¬´ ì ì„ ê²½ìš° ì•ˆì „ì¥ì¹˜ (ìµœì†Œ 1ì¥ì€ ë³´ì¥)
    if min_count == 0:
        print("âš ï¸ ê²½ê³ : í•™ìŠµìš© ë°ì´í„° ë¶„í•  í›„ íŠ¹ì • ì§ˆë³‘ ë°ì´í„°ê°€ 0ê°œì…ë‹ˆë‹¤. ì¬ë¶„í• ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        min_count = 1 

    n_disease = min_count
    n_no_finding = min_count * 2 # ì •ìƒ ë°ì´í„°ëŠ” 2ë°°ìˆ˜ë¡œ
    
    print(f"ğŸ“Š í•™ìŠµ ë°ì´í„° ë°¸ëŸ°ì‹± ê¸°ì¤€: ì§ˆë³‘ {n_disease}ì¥ / ì •ìƒ {n_no_finding}ì¥")
    
    train_fragments = []
    for label in target_classes:
        n_sample = n_no_finding if label == 'No Finding' else n_disease
        
        # ì‹¤ì œ ë°ì´í„°ê°€ ëª©í‘œì¹˜ë³´ë‹¤ ì ìœ¼ë©´ ìˆëŠ” ê±° ë‹¤ ì”€
        actual_count = len(dfs[label])
        if actual_count < n_sample:
            n_sample = actual_count
            
        sampled = dfs[label].sample(n=n_sample, random_state=42)
        train_fragments.append(sampled)
        
    train_df = pd.concat(train_fragments)
    train_df = shuffle(train_df, random_state=42).reset_index(drop=True)
    
    print(f"âœ… Train Set Completed (Balanced): {len(train_df)} images")
    print(f"âœ… Test Set Completed (Imbalanced): {len(test_df)} images")
    
    # Test ë°ì´í„°ì…‹ì— ëª¨ë“  í´ë˜ìŠ¤ê°€ ìµœì†Œ 1ê°œ ì´ìƒ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    print("\n[Test Set í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸]")
    for label in target_classes:
        count = len(test_df[test_df['Finding Labels'].str.contains(label)])
        print(f" - {label}: {count}ì¥")
        if count == 0:
            print(f"âš ï¸ ê²½ê³ : Test Setì— '{label}' ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤! AUC ê³„ì‚° ì‹œ NaNì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    return train_df, test_df

# =========================================================
# [2] Dataset í´ë˜ìŠ¤
#  - íŒŒì¼ëª… ê³µë°± ì œê±° / ì¡´ì¬í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í•„í„°ë§ ê¸°ëŠ¥
# =========================================================
class LungMaskDataset(Dataset):
    def __init__(self, df, img_dir, mask_dir, classes, transform=None):
        self.img_dir = img_dir
        self.mask_dir = mask_dir
        self.transform = transform
        self.classes = classes
        
        # 1. íŒŒì¼ëª… ê³µë°± ì œê±°
        df['Image Index'] = df['Image Index'].astype(str).str.strip()
        
        # 2. [í•µì‹¬] ì‹¤ì œ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” í–‰ë§Œ ë‚¨ê¸°ê¸° (ìœ íš¨ì„± ê²€ì‚¬)
        print(f"ğŸ” ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ ì‹œì‘... (ì´ {len(df)}ê°œ)")
        
        valid_indices = []
        missing_count = 0
        
        for idx in tqdm(range(len(df)), desc="Checking Files"):
            fname = df.iloc[idx]['Image Index']
            file_path = os.path.join(img_dir, fname)
            
            # íŒŒì¼ì´ ì‹¤ì œë¡œ ìˆìœ¼ë©´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            if os.path.exists(file_path):
                valid_indices.append(idx)
            else:
                missing_count += 1
                # ì²˜ìŒ 5ê°œë§Œ ì˜ˆì‹œë¡œ ì¶œë ¥
                if missing_count <= 5:
                    print(f"   [Skip] Missing: {fname}")

        if missing_count > 0:
            print(f"âš ï¸ ì´ {missing_count}ê°œì˜ ì´ë¯¸ì§€ê°€ ì—†ì–´ ì œì™¸í–ˆìŠµë‹ˆë‹¤.")
        
        # ì¡´ì¬í•˜ëŠ” ë°ì´í„°ë§Œ ê°€ì§€ê³  DataFrame ì¬ìƒì„±
        self.df = df.iloc[valid_indices].reset_index(drop=True)
        self.df['labels_list'] = self.df['Finding Labels'].apply(lambda x: x.split('|'))
        
        print(f"âœ… ìµœì¢… ìœ íš¨ ë°ì´í„°ì…‹ í¬ê¸°: {len(self.df)}ê°œ")

    def __len__(self):
        return len(self.df)
    
    def apply_mask_strategy(self, img_path, mask_path):
        try:
            # numpyë¡œ ì½ì–´ì„œ ë””ì½”ë”© (í•œê¸€/íŠ¹ìˆ˜ë¬¸ì/WSL ê²½ë¡œ í˜¸í™˜ì„±)
            img_array = np.fromfile(img_path, np.uint8)
            img = cv2.imdecode(img_array, cv2.IMREAD_GRAYSCALE)
            
            # ë§ˆìŠ¤í¬ê°€ ì—†ìœ¼ë©´ ì›ë³¸ë§Œ ë¦¬í„´ (ìœ ì—°í•œ ì²˜ë¦¬)
            if os.path.exists(mask_path):
                mask_array = np.fromfile(mask_path, np.uint8)
                mask = cv2.imdecode(mask_array, cv2.IMREAD_GRAYSCALE)
            else:
                mask = None
            
        except Exception:
            return None
        
        if img is None: return None
        if mask is None: return Image.fromarray(img).convert('RGB')
        
        # í¬ê¸° ë¶ˆì¼ì¹˜ ë³´ì •
        if img.shape != mask.shape:
            mask = cv2.resize(mask, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)
            
        masked_img = cv2.bitwise_and(img, img, mask=mask)
        
        points = cv2.findNonZero(mask)
        if points is not None:
            x, y, w, h = cv2.boundingRect(points)
            crop_img = masked_img[y:y+h, x:x+w]
        else:
            crop_img = masked_img
            
        return Image.fromarray(crop_img).convert('RGB')

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        fname = row['Image Index']
        
        img_path = os.path.join(self.img_dir, fname)
        mask_path = os.path.join(self.mask_dir, fname)
        
        image = self.apply_mask_strategy(img_path, mask_path)
        
        # ë§Œì•½ ì½ëŠ” ë„ì¤‘ íŒŒì¼ì´ ê¹¨ì ¸ìˆë‹¤ë©´ ê²€ì€ í™”ë©´ ë°˜í™˜ (í•™ìŠµ ì¤‘ë‹¨ ë°©ì§€)
        if image is None:
            image = Image.new('RGB', (224, 224))
            
        if self.transform:
            image = self.transform(image)
            
        label_vec = torch.zeros(len(self.classes), dtype=torch.float32)
        for i, cls_name in enumerate(self.classes):
            if cls_name in row['labels_list']:
                label_vec[i] = 1.0
                
        return image, label_vec
# =========================================================
# [3] ëª¨ë¸ ë° í•™ìŠµ í•¨ìˆ˜
# =========================================================
def get_efficientnet_model(num_classes, device):
    # weights ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²½ê³  ë©”ì‹œì§€ ë°©ì§€ ë° ìµœì‹  ë°©ì‹ ì ìš©
    model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)
    in_features = model.classifier[1].in_features
    model.classifier[1] = nn.Linear(in_features, num_classes)
    return model.to(device)

def train_one_epoch(model, loader, criterion, optimizer, device):
    model.train()
    running_loss = 0.0
    # tqdm ì˜µì…˜ ìˆ˜ì •: Linux í„°ë¯¸ë„ì—ì„œ ê¹¨ì§ ë°©ì§€ (ascii=True, dynamic_ncols=True)
    for inputs, targets in tqdm(loader, desc="Training", leave=False, dynamic_ncols=True):
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    return running_loss / len(loader)

def evaluate(model, loader, device, target_names):
    model.eval()
    all_targets = []
    all_preds = []
    
    with torch.no_grad():
        # tqdm ì˜µì…˜ ìœ ì§€
        for inputs, targets in tqdm(loader, desc="Evaluating", leave=False, dynamic_ncols=True):
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            preds = torch.sigmoid(outputs)
            all_targets.append(targets.cpu().numpy())
            all_preds.append(preds.cpu().numpy())
    
    # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¹ˆ ê°’ ë°˜í™˜
    if not all_targets:
        return 0.0, np.array([]), np.array([])
        
    all_targets = np.vstack(all_targets)
    all_preds = np.vstack(all_preds)
    
    # [ìˆ˜ì •] ì „ì²´ Macro AUC ê³„ì‚° (NaN ë°©ì§€)
    try:
        # y_trueì— í´ë˜ìŠ¤ê°€ í•˜ë‚˜ë°–ì— ì—†ëŠ” ê²½ìš°(ëª¨ë‘ 0 ë˜ëŠ” ëª¨ë‘ 1) ì—ëŸ¬ê°€ ë‚˜ë¯€ë¡œ ì˜ˆì™¸ì²˜ë¦¬
        auc = roc_auc_score(all_targets, all_preds, average='macro')
    except ValueError:
        auc = 0.0  # ê³„ì‚° ë¶ˆê°€ëŠ¥í•  ê²½ìš° 0 ì²˜ë¦¬
        
    return auc, all_targets, all_preds


# =========================================================
# [4] ì„±ëŠ¥í‰ê°€
# =========================================================

def visualize_performance(y_true, y_pred, target_classes, output_dir="./results"):
    import os
    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn.metrics import roc_curve, confusion_matrix, classification_report
    
    # ì €ì¥ í´ë” ìƒì„±
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # ---------------------------------------------------------
    # 1. ROC Curve ì‹œê°í™”
    # ---------------------------------------------------------
    plt.figure(figsize=(10, 8))
    
    for i, label in enumerate(target_classes):
        try:
            fpr, tpr, _ = roc_curve(y_true[:, i], y_pred[:, i])
            
            # [ìˆ˜ì • í•µì‹¬] auc ë³€ìˆ˜ëª… ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•´ metrics.aucë¡œ ëª…ì‹œì  í˜¸ì¶œ
            roc_auc_val = metrics.auc(fpr, tpr) 
            
            plt.plot(fpr, tpr, lw=2, label=f'{label} (AUC = {roc_auc_val:.4f})')
        except Exception as e:
            print(f"âš ï¸ {label} ROC Curve ìƒì„± ì‹¤íŒ¨: {e}")

    plt.plot([0, 1], [0, 1], 'k--', lw=2) 
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate (FPR)')
    plt.ylabel('True Positive Rate (TPR)')
    plt.title('Multi-label ROC Curves')
    plt.legend(loc="lower right")
    plt.grid(alpha=0.3)
    
    save_path_roc = os.path.join(output_dir, "roc_curve.png")
    plt.savefig(save_path_roc)
    print(f"ğŸ“ˆ ROC Curve ì €ì¥ ì™„ë£Œ: {save_path_roc}")
    # plt.show() # í•„ìš” ì‹œ ì£¼ì„ í•´ì œ

    # ---------------------------------------------------------
    # 2. Confusion Matrix ì‹œê°í™”
    # ---------------------------------------------------------
    threshold = 0.5
    y_pred_binary = (y_pred > threshold).astype(int)

    n_classes = len(target_classes)
    cols = 2
    rows = (n_classes + 1) // 2
    
    fig, axes = plt.subplots(rows, cols, figsize=(12, 5 * rows))
    axes = axes.flatten()

    for i, label in enumerate(target_classes):
        try:
            cm = confusion_matrix(y_true[:, i], y_pred_binary[:, i])
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i], 
                        xticklabels=['Negative', 'Positive'], 
                        yticklabels=['Negative', 'Positive'])
            axes[i].set_title(f'Confusion Matrix - {label}')
            axes[i].set_ylabel('Actual')
            axes[i].set_xlabel('Predicted')
        except Exception as e:
            print(f"âš ï¸ {label} Confusion Matrix ìƒì„± ì‹¤íŒ¨: {e}")

    for j in range(i + 1, len(axes)):
        axes[j].axis('off')

    plt.tight_layout()
    save_path_cm = os.path.join(output_dir, "confusion_matrices.png")
    plt.savefig(save_path_cm)
    print(f"ğŸ“‰ Confusion Matrices ì €ì¥ ì™„ë£Œ: {save_path_cm}")
    # plt.show()

    # ---------------------------------------------------------
    # 3. í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸
    # ---------------------------------------------------------
    print("\n" + "="*50)
    print("ğŸ“‹ Classification Report (Threshold = 0.5)")
    print("="*50)
    print(classification_report(y_true, y_pred_binary, target_names=target_classes, zero_division=0))






# =========================================================
# [Main Execution Flow]
# =========================================================
if __name__ == "__main__":
    # --- [ìˆ˜ì •] WSL ê²½ë¡œ ì„¤ì • ---
    # Windowsì˜ D: ë“œë¼ì´ë¸Œ -> /mnt/d
    # Windowsì˜ C: ë“œë¼ì´ë¸Œ -> /mnt/c
    RAW_IMG_DIR = "/mnt/d/lung_xray/final_denoised"          
    MASK_DIR = "/mnt/d/lung_xray/final_contour_masks" 
    CSV_PATH = "/mnt/d/lung_xray/Data_Entry_processed_Final.csv" 
    
    TARGET_CLASSES = ['No Finding', 'Infiltration', 'Effusion', 'Atelectasis'] 
    
    # [ìˆ˜ì •] CPU í•™ìŠµ íš¨ìœ¨ì„ ìœ„í•´ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¦ê°€ (RAM ë¶€ì¡± ì‹œ 16ìœ¼ë¡œ ì¡°ì ˆ)
    BATCH_SIZE = 32 
    EPOCHS = 5
    LR = 1e-4
    
    train_df, test_df = prepare_datasets(CSV_PATH, TARGET_CLASSES)
    
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    train_ds = LungMaskDataset(train_df, RAW_IMG_DIR, MASK_DIR, TARGET_CLASSES, transform)
    test_ds = LungMaskDataset(test_df, RAW_IMG_DIR, MASK_DIR, TARGET_CLASSES, transform)
    
    # [ìˆ˜ì •] Linux/WSL í™˜ê²½ì—ì„œëŠ” num_workersë¥¼ ë†’ì—¬ì•¼ CPUê°€ ì‰¬ì§€ ì•Šê³  ì¼í•©ë‹ˆë‹¤.
    # ì•ˆì „í•˜ê²Œ CPU ì „ì²´ ì½”ì–´ ìˆ˜ ì‚¬ìš©
    num_workers = multiprocessing.cpu_count()
    print(f"ğŸš€ Workers setting: {num_workers} cores")

    # [ìˆ˜ì •] pin_memory=False (CPU í•™ìŠµ ì‹œì—ëŠ” Falseê°€ ì˜¤ë²„í—¤ë“œê°€ ì ìŒ)
    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=False)
    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=False)
    
    device = torch.device("cpu") # ëª…ì‹œì ìœ¼ë¡œ CPU ì§€ì •
    print(f"ğŸš€ Device: {device} (Optimized for Multi-core)")
    
    model = get_efficientnet_model(len(TARGET_CLASSES), device)
    criterion = nn.BCEWithLogitsLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)
    
    # í•™ìŠµ ì‹œì‘
    print("ğŸš€ Training Start...")
    
    # ëª¨ë¸ ì €ì¥ ê²½ë¡œ ì„¤ì •
    SAVE_PATH = "./best_model.pth"
    best_auc = 0.0

    for epoch in range(EPOCHS):
        loss = train_one_epoch(model, train_loader, criterion, optimizer, device)
        
        # í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ ì„±ëŠ¥ í‰ê°€
        auc, _, _ = evaluate(model, test_loader, device, TARGET_CLASSES)
        print(f"Epoch [{epoch+1}/{EPOCHS}] Loss: {loss:.4f} | Test AUC: {auc:.4f}")
        
        # [ìˆ˜ì • 2] ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì„ ë•Œ(ë˜ëŠ” ë§¤ ì—í­ë§ˆë‹¤) ëª¨ë¸ ì €ì¥ ì½”ë“œ ì¶”ê°€
        # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ ë§¤ ì—í­ë§ˆë‹¤ ë®ì–´ì“°ê±°ë‚˜, AUCê°€ ê°±ì‹ ë  ë•Œ ì €ì¥í•©ë‹ˆë‹¤.
        if auc > best_auc:
            best_auc = auc
            torch.save(model.state_dict(), SAVE_PATH)
            print(f"  ğŸ’¾ Model Saved! (Best AUC: {best_auc:.4f})")

    print(f"\nâœ… í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {SAVE_PATH}")
    
    # ---------------------------------------------------------
    # ì €ì¥ëœ ëª¨ë¸ì„ ë‹¤ì‹œ ë¶ˆëŸ¬ì™€ì„œ ìµœì¢… í‰ê°€ (ì„ íƒ ì‚¬í•­)
    # ---------------------------------------------------------
    model.load_state_dict(torch.load(SAVE_PATH)) 
    
    print("\n====[Final Report]====")
    final_auc, y_true, y_pred = evaluate(model, test_loader, device, TARGET_CLASSES)
    # ... (ì´í•˜ ì‹œê°í™” ì½”ë“œ ë™ì¼) ...




    print("\n====[Final Report]====")
    # 1. í‰ê°€ ìˆ˜í–‰
    final_auc, y_true, y_pred = evaluate(model, test_loader, device, TARGET_CLASSES)
    
    # 2. ê²°ê³¼ í…ìŠ¤íŠ¸ ì¶œë ¥
    print(f"Overall Macro AUC: {final_auc:.4f}")
    
    for i, cls in enumerate(TARGET_CLASSES):
        try:
            # ì•ˆì „í•œ AUC ê³„ì‚°
            if len(np.unique(y_true[:, i])) > 1:
                cls_auc = roc_auc_score(y_true[:, i], y_pred[:, i])
                print(f" - {cls:<15} AUC: {cls_auc:.4f}")
            else:
                print(f" - {cls:<15} AUC: N/A (ë°ì´í„° ë¶€ì¡±)")
        except:
            print(f" - {cls:<15} AUC: Error")
            
    # =========================================================
    # [ì¶”ê°€] 3. ì‹œê°í™” í•¨ìˆ˜ í˜¸ì¶œ
    # =========================================================
    # y_true, y_predëŠ” evaluate í•¨ìˆ˜ì—ì„œ ì´ë¯¸ numpy arrayë¡œ ë³€í™˜ë˜ì–´ ë‚˜ì˜´
    visualize_performance(y_true, y_pred, TARGET_CLASSES, output_dir="./results")



